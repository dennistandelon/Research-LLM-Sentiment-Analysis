{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":251460335,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q openai google-generativeai anthropic statsmodels scikit-learn pandas tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:00:06.467488Z","iopub.execute_input":"2025-07-22T11:00:06.467904Z","iopub.status.idle":"2025-07-22T11:00:10.567215Z","shell.execute_reply.started":"2025-07-22T11:00:06.467877Z","shell.execute_reply":"2025-07-22T11:00:10.565887Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom statsmodels.stats.contingency_tables import mcnemar\nfrom tqdm import tqdm\nimport os\n\nfrom openai import OpenAI\nimport google.generativeai as genai\nimport anthropic\n\nfrom kaggle_secrets import UserSecretsClient","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:00:10.568881Z","iopub.execute_input":"2025-07-22T11:00:10.569270Z","iopub.status.idle":"2025-07-22T11:00:10.575551Z","shell.execute_reply.started":"2025-07-22T11:00:10.569238Z","shell.execute_reply":"2025-07-22T11:00:10.574442Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ==== Setup API Keys ====\nuser_secrets = UserSecretsClient()\n\nos.environ[\"OPENAI_API_KEY\"] = user_secrets.get_secret(\"OPENAI_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"ANTHROPIC_API_KEY\"] = user_secrets.get_secret(\"ANTHROPIC_API_KEY\")\n\n# ==== Initialize Clients ====\nopenai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\nclaude_client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\n# ==== Dataset Paths ====\nTRAIN_PATH = \"/kaggle/input/paper-kelulusan-eda/train_data.csv\"  # for few-shot\nTEST_PATH = \"/kaggle/input/paper-kelulusan-eda/test_data.csv\"    # for evaluation\n\nLLMS = [\n    \"gpt\", \n    \"gemini\", \n    \"claude\"\n]\nAPPROACHES = [\"zero_shot\", \"few_shot\", \"chain_of_thought\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:00:10.576653Z","iopub.execute_input":"2025-07-22T11:00:10.577287Z","iopub.status.idle":"2025-07-22T11:00:11.232148Z","shell.execute_reply.started":"2025-07-22T11:00:10.577261Z","shell.execute_reply":"2025-07-22T11:00:11.231162Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ==== LLM Calls ====\ndef call_gpt(prompt: str) -> str:\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0\n    )\n    return response.choices[0].message.content\n\ndef call_gemini(prompt: str) -> str:\n    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n    response = model.generate_content(prompt)\n    return response.text\n\ndef call_claude(prompt: str) -> str:\n    response = claude_client.messages.create(\n        model=\"claude-3-5-sonnet-20240620\",\n        max_tokens=300,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.content[0].text\n\nLLM_CALLS = {\n    \"gpt\": call_gpt,\n    \"gemini\": call_gemini,\n    \"claude\": call_claude,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:00:11.234472Z","iopub.execute_input":"2025-07-22T11:00:11.234855Z","iopub.status.idle":"2025-07-22T11:00:11.242647Z","shell.execute_reply.started":"2025-07-22T11:00:11.234830Z","shell.execute_reply":"2025-07-22T11:00:11.241367Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def chunk_list(lst, chunk_size=5):\n    \"\"\"Split list into chunks for bulk prediction.\"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i:i + chunk_size]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:00:11.243750Z","iopub.execute_input":"2025-07-22T11:00:11.244073Z","iopub.status.idle":"2025-07-22T11:00:11.263117Z","shell.execute_reply.started":"2025-07-22T11:00:11.244048Z","shell.execute_reply":"2025-07-22T11:00:11.262052Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def bulk_predict(llm_name, approach, texts, train_df=None, batch_size=50):\n    predictions = []\n    for batch in chunk_list(texts, batch_size):\n        text_list_str = \"\\n\".join([f\"{i+1}. {t}\" for i, t in enumerate(batch)])\n        \n        # Build prompt\n        if approach == \"zero_shot\":\n            prompt = (\n                \"Lakukan klasifikasi sentimen (kita sebut positif sebagai positive dan negatif sebagai negative) untuk setiap teks dibawah ini:\\n\"\n                f\"{text_list_str}\\n\"\n                \"Berikan hasil dalam format:\\n1. positive\\n2. negative\\n...\"\n            )\n        elif approach == \"few_shot\":\n            fewshot_examples = \"\"\n            for _, row in train_df.sample(n=5, random_state=42).iterrows():\n                fewshot_examples += f\"Text: '{row['clean_text']}' -> Sentiment: {row['sentiment'].lower()}\\n\"\n            prompt = (\n                \"Lakukan klasifikasi sentimen, berikut beberapa contoh samplenya:\\n\"\n                f\"{fewshot_examples}\\n\"\n                f\"Sekarang, lakukan klasifikasi dalam teks berikut:\\n{text_list_str}\\n\"\n                \"Berikan hasil dalam format:\\n1. positive\\n2. negative\\n...\"\n            )\n        else:  # chain_of_thought\n            prompt = (\n                \"Analisa dan lakukan klasifikasisetiap teks sebagai positif atau negatif:\\n\"\n                f\"{text_list_str}\\n\"\n                \"Berikan hasil akhir saja dalam format:\\n1. positive\\n2. negative\\n...\"\n            )\n\n        raw_output = LLM_CALLS[llm_name](prompt)\n        raw_lines = [line.strip().lower() for line in raw_output.split(\"\\n\") if line.strip()]\n\n        # Parse predictions line-by-line, fallback to neutral\n        batch_preds = []\n        for i in range(len(batch)):\n            if i < len(raw_lines) and (\"positive\" in raw_lines[i] or \"negative\" in raw_lines[i]):\n                sentiment = \"positive\" if \"positive\" in raw_lines[i] else \"negative\"\n            else:\n                sentiment = \"negative\"  # fallback\n            batch_preds.append(sentiment)\n\n        predictions.extend(batch_preds)\n    \n    # Ensure prediction count matches texts count\n    assert len(predictions) == len(texts), f\"Prediction length mismatch: {len(predictions)} vs {len(texts)}\"\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:00:11.264441Z","iopub.execute_input":"2025-07-22T11:00:11.264818Z","iopub.status.idle":"2025-07-22T11:00:11.282062Z","shell.execute_reply.started":"2025-07-22T11:00:11.264764Z","shell.execute_reply":"2025-07-22T11:00:11.281089Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    return {\n        \"accuracy\": accuracy_score(y_true, y_pred),\n        \"precision\": precision_score(y_true, y_pred, average='binary', pos_label='positive'),\n        \"recall\": recall_score(y_true, y_pred, average='binary', pos_label='positive'),\n        \"f1\": f1_score(y_true, y_pred, average='binary', pos_label='positive'),\n        \"report\": classification_report(y_true, y_pred, zero_division=0)\n    }\n\ndef mcnemar_test(y_true, y_pred_1, y_pred_2):\n    table = [[0, 0], [0, 0]]\n    for gt, p1, p2 in zip(y_true, y_pred_1, y_pred_2):\n        correct1 = p1 == gt\n        correct2 = p2 == gt\n        table[int(not correct1)][int(not correct2)] += 1\n    return mcnemar(table, exact=True).pvalue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:00:11.283329Z","iopub.execute_input":"2025-07-22T11:00:11.283730Z","iopub.status.idle":"2025-07-22T11:00:11.304631Z","shell.execute_reply.started":"2025-07-22T11:00:11.283702Z","shell.execute_reply":"2025-07-22T11:00:11.303607Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:00:11.306187Z","iopub.execute_input":"2025-07-22T11:00:11.306558Z","iopub.status.idle":"2025-07-22T11:00:11.382465Z","shell.execute_reply.started":"2025-07-22T11:00:11.306522Z","shell.execute_reply":"2025-07-22T11:00:11.381455Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# === Predictions ===\ny_true = test_df[\"sentiment\"].str.lower()\n\nresults = {}\nmetrics = {}\n\nfor llm in LLMS:\n    results[llm] = {}\n    metrics[llm] = {}\n    for approach in APPROACHES:\n        print(f\"\\nRunning {llm.upper()} - {approach.upper()}...\")\n        preds = bulk_predict(\n            llm_name=llm,\n            approach=approach,\n            texts=test_df[\"clean_text\"].tolist(),\n            train_df=train_df\n        )\n        results[llm][approach] = preds\n        metrics[llm][approach] = evaluate(y_true, preds)\n        print(metrics[llm][approach][\"report\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:00:11.383543Z","iopub.execute_input":"2025-07-22T11:00:11.383885Z","iopub.status.idle":"2025-07-22T11:25:58.151462Z","shell.execute_reply.started":"2025-07-22T11:00:11.383855Z","shell.execute_reply":"2025-07-22T11:25:58.150090Z"}},"outputs":[{"name":"stdout","text":"\nRunning GPT - ZERO_SHOT...\n              precision    recall  f1-score   support\n\n    negative       0.86      0.65      0.74       500\n    positive       0.72      0.89      0.80       500\n\n    accuracy                           0.77      1000\n   macro avg       0.79      0.77      0.77      1000\nweighted avg       0.79      0.77      0.77      1000\n\n\nRunning GPT - FEW_SHOT...\n              precision    recall  f1-score   support\n\n    negative       0.87      0.67      0.76       500\n    positive       0.73      0.90      0.81       500\n\n    accuracy                           0.79      1000\n   macro avg       0.80      0.79      0.78      1000\nweighted avg       0.80      0.79      0.78      1000\n\n\nRunning GPT - CHAIN_OF_THOUGHT...\n              precision    recall  f1-score   support\n\n    negative       0.88      0.64      0.75       500\n    positive       0.72      0.92      0.81       500\n\n    accuracy                           0.78      1000\n   macro avg       0.80      0.78      0.78      1000\nweighted avg       0.80      0.78      0.78      1000\n\n\nRunning GEMINI - ZERO_SHOT...\n              precision    recall  f1-score   support\n\n    negative       0.90      0.66      0.77       500\n    positive       0.73      0.93      0.82       500\n\n    accuracy                           0.80      1000\n   macro avg       0.82      0.80      0.79      1000\nweighted avg       0.82      0.80      0.79      1000\n\n\nRunning GEMINI - FEW_SHOT...\n              precision    recall  f1-score   support\n\n    negative       0.89      0.65      0.75       500\n    positive       0.72      0.92      0.81       500\n\n    accuracy                           0.78      1000\n   macro avg       0.81      0.78      0.78      1000\nweighted avg       0.81      0.78      0.78      1000\n\n\nRunning GEMINI - CHAIN_OF_THOUGHT...\n              precision    recall  f1-score   support\n\n    negative       0.90      0.65      0.75       500\n    positive       0.72      0.93      0.81       500\n\n    accuracy                           0.79      1000\n   macro avg       0.81      0.79      0.78      1000\nweighted avg       0.81      0.79      0.78      1000\n\n\nRunning CLAUDE - ZERO_SHOT...\n              precision    recall  f1-score   support\n\n    negative       0.90      0.64      0.75       500\n    positive       0.72      0.93      0.81       500\n\n    accuracy                           0.78      1000\n   macro avg       0.81      0.78      0.78      1000\nweighted avg       0.81      0.78      0.78      1000\n\n\nRunning CLAUDE - FEW_SHOT...\n              precision    recall  f1-score   support\n\n    negative       0.91      0.72      0.80       500\n    positive       0.77      0.93      0.84       500\n\n    accuracy                           0.82      1000\n   macro avg       0.84      0.82      0.82      1000\nweighted avg       0.84      0.82      0.82      1000\n\n\nRunning CLAUDE - CHAIN_OF_THOUGHT...\n              precision    recall  f1-score   support\n\n    negative       0.90      0.68      0.77       500\n    positive       0.74      0.92      0.82       500\n\n    accuracy                           0.80      1000\n   macro avg       0.82      0.80      0.80      1000\nweighted avg       0.82      0.80      0.80      1000\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# === McNemar Tests (LLM vs LLM for each approach) ===\nfor approach in APPROACHES:\n    print(f\"\\n=== McNemar Tests for {approach.upper()} ===\")\n    p_gpt_claude = mcnemar_test(y_true, results[\"gpt\"][approach], results[\"claude\"][approach])\n    p_gpt_gemini = mcnemar_test(y_true, results[\"gpt\"][approach], results[\"gemini\"][approach])\n    p_claude_gemini = mcnemar_test(y_true, results[\"claude\"][approach], results[\"gemini\"][approach])\n    print(f\"GPT vs Claude: p={p_gpt_claude:.4f}\")\n    print(f\"GPT vs Gemini: p={p_gpt_gemini:.4f}\")\n    print(f\"Claude vs Gemini: p={p_claude_gemini:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:25:58.154396Z","iopub.execute_input":"2025-07-22T11:25:58.154710Z","iopub.status.idle":"2025-07-22T11:25:58.173131Z","shell.execute_reply.started":"2025-07-22T11:25:58.154689Z","shell.execute_reply":"2025-07-22T11:25:58.171891Z"}},"outputs":[{"name":"stdout","text":"\n=== McNemar Tests for ZERO_SHOT ===\nGPT vs Claude: p=0.1337\nGPT vs Gemini: p=0.0001\nClaude vs Gemini: p=0.0595\n\n=== McNemar Tests for FEW_SHOT ===\nGPT vs Claude: p=0.0000\nGPT vs Gemini: p=0.7838\nClaude vs Gemini: p=0.0000\n\n=== McNemar Tests for CHAIN_OF_THOUGHT ===\nGPT vs Claude: p=0.2521\nGPT vs Gemini: p=0.6866\nClaude vs Gemini: p=0.2429\n","output_type":"stream"}],"execution_count":18}]}